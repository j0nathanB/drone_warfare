{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from thefuzz import process\n",
    "import re\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import multiprocessing as mp\n",
    "\n",
    "DIR = \"/Users/tlahtolli/dev/drone_warfare/data/cleanup\"\n",
    "NGA_DIR = \"/Users/tlahtolli/dev/drone_warfare/data/nga\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tlahtolli/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:3378: DtypeWarning: Columns (8,9,13,16,17,18,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "af = pd.read_csv(f'{DIR}/3_manual_cleanup/af.csv',)\n",
    "pk = pd.read_csv(f'{DIR}/3_manual_cleanup/pk.csv')\n",
    "so = pd.read_csv(f'{DIR}/3_manual_cleanup/so.csv')\n",
    "ye = pd.read_csv(f'{DIR}/3_manual_cleanup/ye.csv')\n",
    "\n",
    "af_nga = pd.read_csv(f'{NGA_DIR}/Afghanistan/Afghanistan.txt', sep='\\t')\n",
    "pk_nga = pd.read_csv(f'{NGA_DIR}/Pakistan/Pakistan.txt', sep='\\t')\n",
    "so_nga = pd.read_csv(f'{NGA_DIR}/Somalia/Somalia.txt', sep='\\t')\n",
    "ye_nga = pd.read_csv(f'{NGA_DIR}/Yemen/Yemen.txt', sep='\\t')\n",
    "\n",
    "admin_nga = pd.read_csv(f'{DIR}/nga/Administrative_Regions/Administrative_Regions.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationNormalizer:\n",
    "    def __init__(self, source_df, ref_df):\n",
    "        self.source_df = source_df.copy()\n",
    "        self.ref_df = ref_df.copy()\n",
    "        self.target_df = None\n",
    "        self.source_subset = None\n",
    "        self.ref_subset = None\n",
    "        self.adm1_codes = []\n",
    "\n",
    "    def create_sort_name(self, name):\n",
    "        name = re.sub(r'[^a-zA-Z0-9\\s]', '', name)\n",
    "        return name.upper().replace(' ', '')\n",
    "    \n",
    "    def create_sort_columns(self, df, col):\n",
    "        df[f'sort_{col}'] = df[col].apply(self.create_sort_name)\n",
    "        return df\n",
    "    \n",
    "    def get_unique_source_locations(self, df, col='Adm_1', adm1_code=None):\n",
    "        unwanted_values = [\"unknown\", \"unclear\", \"various\", \"multiple\", 'Unknown', 'Unclear', 'Various', 'Multiple']\n",
    "        filtered_df = df[~df[col].isin(unwanted_values) & df[col].notnull()]\n",
    "\n",
    "        if adm1_code:\n",
    "            adm1_col = 'matched_sort_Adm_1_adm1'\n",
    "            filtered_df = filtered_df[filtered_df[adm1_col] == adm1_code]\n",
    "        result = filtered_df[[col]].drop_duplicates()\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def get_unique_ref_location_data(self, desig_cd='ADM1', adm1_code=None):\n",
    "        if desig_cd == '':\n",
    "            filter_condition = (~self.ref_df['desig_cd'].isin(['ADM1', 'ADM2', 'ADM3', 'ADM4']))\n",
    "        else:\n",
    "            filter_condition = (self.ref_df['desig_cd'] == desig_cd) & (self.ref_df['name_rank'] == 1)\n",
    "\n",
    "        if adm1_code:\n",
    "            filter_condition &= (self.ref_df['adm1'] == adm1_code)\n",
    "\n",
    "        unique_adm1_df = self.ref_df.loc[filter_condition, ['ufi', 'adm1', 'sort_name', 'full_name','full_nm_nd', 'desig_cd','lat_dd', 'long_dd']].drop_duplicates(subset=['sort_name'])\n",
    "        return unique_adm1_df\n",
    "\n",
    "    def fuzzy_search(self, query, choices):\n",
    "        try:\n",
    "            match = process.extractOne(query, choices)\n",
    "            if match[1] >= 80:\n",
    "                return match[0]\n",
    "            else:\n",
    "                return 'unclear'\n",
    "        except:\n",
    "            return 'unclear'\n",
    "        \n",
    "\n",
    "    def get_matches(self, source_subset, ref_subset, col='Adm_1'):\n",
    "        target_col = f'matched_sort_{col}'\n",
    "        # Rename the sort_name column to match the col\n",
    "        ref_subset = ref_subset.rename(columns={'sort_name': f'{target_col}'})\n",
    "        # Convert the Pandas DataFrames to Dask DataFrames\n",
    "        source_ddf = dd.from_pandas(source_subset, npartitions=8)\n",
    "        ref_ddf = dd.from_pandas(ref_subset, npartitions=8)\n",
    "\n",
    "        def fuzzy_search_sort_name(query, ref_ddf):\n",
    "            return self.fuzzy_search(query, ref_ddf[f'{target_col}'])\n",
    "\n",
    "        # Apply the fuzzy search function on the source subset searching the reference subset\n",
    "        source_ddf[f'{target_col}'] = source_ddf[f'sort_{col}'].apply(fuzzy_search_sort_name, ref_ddf=ref_ddf, meta=(f'{target_col}', 'object'))\n",
    "\n",
    "        # Convert the Dask DataFrame back to a Pandas DataFrame\n",
    "        source_df = source_ddf.compute()\n",
    "\n",
    "        # Rename columns in ref_subset by prepending target_col\n",
    "        renamed_columns = {col: f'{target_col}_{col}' for col in ref_subset.columns}\n",
    "        ref_subset = ref_subset.rename(columns=renamed_columns)\n",
    "\n",
    "        # Perform the left join between source_df and ref_subset\n",
    "        result_df = pd.merge(source_df, ref_subset, left_on=f'{target_col}', right_on=f'{target_col}_{target_col}', how='left')\n",
    "        \n",
    "        # Return the original columns along with the matched columns\n",
    "        return result_df[[col, *result_df.columns[result_df.columns.str.startswith(target_col)]]]\n",
    "\n",
    "\n",
    "    def get_normalized_loc_data(self, col='Adm_1'):\n",
    "        self.create_sort_columns(self.source_subset, col)\n",
    "\n",
    "        normalized_names = self.get_matches(self.source_subset, self.ref_subset, col)\n",
    "        return normalized_names\n",
    "\n",
    "    def normalize(self, df, col=\"Adm_1\", desig_cd='ADM1'):\n",
    "        normalized_data = self.get_normalized_loc_data(col)\n",
    "        merged_df = pd.merge(df, normalized_data, on=col, how='left', suffixes=('', '_matched'))\n",
    "\n",
    "        # Update the original columns with matched values\n",
    "        for col in df.columns:\n",
    "            if f\"{col}_matched\" in merged_df.columns:\n",
    "                # Update the original column with the matched values when they exist (i.e., not NaN)\n",
    "                merged_df[col].update(merged_df[f\"{col}_matched\"].dropna())\n",
    "\n",
    "        # Drop the extra columns created due to the merge\n",
    "        cols_to_keep = [col for col in merged_df.columns if not col.endswith('_matched')]\n",
    "        merged_df = merged_df[cols_to_keep]\n",
    "\n",
    "        return merged_df\n",
    "\n",
    "    def get_unique_adm1_codes(self, df):\n",
    "        return df['matched_sort_Adm_1_adm1'].dropna().drop_duplicates().tolist()\n",
    "    \n",
    "    def prepare_data(self, df, col='Adm_1', desig_cd='ADM1', adm1_code=None):\n",
    "        self.source_subset = self.get_unique_source_locations(df, col, adm1_code)\n",
    "        self.ref_subset = self.get_unique_ref_location_data(desig_cd, adm1_code)\n",
    "\n",
    "    def normalize_adm_1(self):\n",
    "        self.prepare_data(self.source_df, 'Adm_1', 'ADM1')\n",
    "        self.target_df = self.normalize(self.source_df, 'Adm_1', 'ADM1')\n",
    "        self.adm1_codes = self.get_unique_adm1_codes(self.target_df)\n",
    "    \n",
    "    def normalize_remaining(self, col='Adm_2', desig_cd='ADM2'):\n",
    "        for adm1_code in self.adm1_codes:\n",
    "            self.prepare_data(self.target_df, col, desig_cd, adm1_code)\n",
    "            self.target_df = self.normalize(self.target_df, col, desig_cd)\n",
    "\n",
    "    def output(self, filename):\n",
    "        self.target_df.to_csv(filename, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 12:14:49,607 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.32 GiB -- Worker memory limit: 1.86 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PK Done\n"
     ]
    }
   ],
   "source": [
    "with LocalCluster(n_workers=int(0.5 * mp.cpu_count()),\n",
    "    processes=True,\n",
    "    threads_per_worker=1,\n",
    "    memory_limit='2GB',\n",
    ") as cluster, Client(cluster) as client:\n",
    "    # Do something using 'client'\n",
    "    af_norm = LocationNormalizer(af, af_nga)\n",
    "    af_norm.normalize_adm_1()\n",
    "    af_norm.normalize_remaining('Adm_2', 'ADM2')\n",
    "    af_norm.normalize_remaining('Loc', '')\n",
    "    af_norm.output(f'{DIR}/4_nga_normalize/AF.csv')\n",
    "    print('AF Done')\n",
    "\n",
    "with LocalCluster(n_workers=int(0.5 * mp.cpu_count()),\n",
    "    processes=True,\n",
    "    threads_per_worker=1,\n",
    "    memory_limit='2GB',\n",
    ") as cluster, Client(cluster) as client:\n",
    "    # Do something using 'client'\n",
    "    pk_norm = LocationNormalizer(pk, pk_nga)\n",
    "    pk_norm.normalize_adm_1()\n",
    "    pk_norm.normalize_remaining('Adm_2', 'ADM3') # not designated as ADM2 in NGA but ADM3\n",
    "    # pk_norm.normalize_remaining('Adm_3', '') # dask crashes with this, run separately\n",
    "    # pk_norm.normalize_remaining('Loc', '') # dask crashes with this, run separately\n",
    "    pk_norm.output(f'{DIR}/4_nga_normalize/PK.csv')\n",
    "    print('PK Done')\n",
    "\n",
    "with LocalCluster(n_workers=int(0.5 * mp.cpu_count()),\n",
    "    processes=True,\n",
    "    threads_per_worker=1,\n",
    "    memory_limit='2GB',\n",
    ") as cluster, Client(cluster) as client:\n",
    "    # Do something using 'client'\n",
    "    so_norm = LocationNormalizer(so, so_nga)\n",
    "    so_norm.normalize_adm_1()\n",
    "    so_norm.normalize_remaining('Adm_2', '')\n",
    "    so_norm.normalize_remaining('Loc', '')\n",
    "    so_norm.output(f'{DIR}/4_nga_normalize/SO.csv')\n",
    "    print('SO Done')\n",
    "\n",
    "with LocalCluster(n_workers=int(0.5 * mp.cpu_count()),\n",
    "    processes=True,\n",
    "    threads_per_worker=1,\n",
    "    memory_limit='2GB',\n",
    ") as cluster, Client(cluster) as client:\n",
    "    # Do something using 'client'\n",
    "    ye_norm = LocationNormalizer(ye, ye_nga)\n",
    "    ye_norm.normalize_adm_1()\n",
    "    ye_norm.normalize_remaining('Adm_2', 'ADM2')\n",
    "    ye_norm.normalize_remaining('Loc', '')\n",
    "    ye_norm.output(f'{DIR}/4_nga_normalize/YE.csv')\n",
    "    print('YE Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PK Done\n"
     ]
    }
   ],
   "source": [
    "pk = pd.read_csv(f'{DIR}/4_nga_normalize/PK.csv')\n",
    "\n",
    "with LocalCluster(n_workers=int(0.5 * mp.cpu_count()),\n",
    "    processes=True,\n",
    "    threads_per_worker=1,\n",
    "    memory_limit='2GB',\n",
    ") as cluster, Client(cluster) as client:\n",
    "    # Do something using 'client'\n",
    "    pk_norm = LocationNormalizer(pk, pk_nga)\n",
    "    pk_norm.adm1_codes = pk_norm.get_unique_adm1_codes(pk_norm.source_df)\n",
    "    pk_norm.target_df = pk_norm.source_df.copy()\n",
    "    pk_norm.normalize_remaining('Adm_3', '') # not designated as ADM3 in NGA\n",
    "    pk_norm.normalize_remaining('Loc', '')\n",
    "    pk_norm.output(f'{DIR}/4_nga_normalize/PK.csv')\n",
    "    print('PK Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
